{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqETy/oopmpSGVKwZSNtC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliana-zh/ML/blob/main/NLP_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели с использованием представления слов (embeddings)"
      ],
      "metadata": {
        "id": "b5JU3g9iIVsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем датасет из новостных текстов, состоящих из 20 групп"
      ],
      "metadata": {
        "id": "9KUYC4tid-GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['alt.atheism', 'talk.religion.misc',\n",
        "              'comp.graphics', 'sci.space']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                      categories=categories)\n",
        "newsgroups_train.filenames.shape"
      ],
      "metadata": {
        "id": "AOfebOAAaO3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06edad9-561f-4701-b56a-a73e36032e74"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2034,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy-zZIQiaZy0",
        "outputId": "25564b9b-430f-4bcb-9b95-0ebac5ce50a5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем предварительно натренированные представления слов:"
      ],
      "metadata": {
        "id": "xAP0KbNwLM5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "embeddings_pretrained = api.load('glove-twitter-25')"
      ],
      "metadata": {
        "id": "-ToiMDSQIh5o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждого текста из нашей тренировочной выборки получим набор слов.\n",
        "По набору слов с помощью технологии Word2Vec получим представления этих слов.\n"
      ],
      "metadata": {
        "id": "2GOP7_ijL6zg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция, отвечающая за предобработку:"
      ],
      "metadata": {
        "id": "OXYd3FxXhbG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# стоп-слова, preproc\n",
        "\n",
        "stopWords = set(stopwords.words('english'))\n",
        "nltk.download('wordnet')\n",
        "wnl = nltk.WordNetLemmatizer()\n",
        "\n",
        "def preproc_nltk(text):\n",
        "    #text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
        "    return ' '.join([wnl.lemmatize(word) for word in word_tokenize(text.lower()) if word not in stopWords])\n",
        "\n",
        "st = \"Oh, I think I ve landed Where there are miracles at work,  For the thirst and for the hunger Come the conference of birds\"\n",
        "preproc_nltk(st)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_HnKZwSnVsuR",
        "outputId": "a7095104-fbd7-4860-ba82-6e0794d2ba11"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'oh , think landed miracle work , thirst hunger come conference bird'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь мы работаем над вариантом, когда мы используем технологию Word2Vec, чтобы подсчитать представления слов."
      ],
      "metadata": {
        "id": "xOR_vjrae_to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "proc_words = [preproc_nltk(text).split() for text in newsgroups_train.data]\n",
        "embeddings_trained = Word2Vec(proc_words, # data for model to train on\n",
        "                 vector_size=100,         # embedding vector size\n",
        "                 min_count=3,             # consider words that occured at least 5 times\n",
        "                 window=3).wv"
      ],
      "metadata": {
        "id": "p-JbwX2tIktw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь мы работаем над вариантом, когда нам *даны заранее подсчитанные представления слов*. Создаем функцию, которая является отображением из какого-либо комментария в сумму векторов, являющихся представлениями слов комментария:"
      ],
      "metadata": {
        "id": "QwnHT3AAUJXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sum(comment, embeddings):\n",
        "    \"\"\"\n",
        "    implement a function that converts preprocessed comment to a sum of token vectors\n",
        "    \"\"\"\n",
        "    embedding_dim = embeddings.vectors.shape[1]\n",
        "    features = np.zeros([embedding_dim], dtype='float32')\n",
        "\n",
        "    for word in preproc_nltk(comment).split():\n",
        "        if word in embeddings:\n",
        "            features += embeddings[f'{word}']\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "L1EoqB34IoXw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings_trained.index_to_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKPPU_p6IrSh",
        "outputId": "0718e65e-2476-41f5-f17e-71999a5f063b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13566"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разбиваем выборку на обучающую и тестовую"
      ],
      "metadata": {
        "id": "aEn6bGdhfq_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_wv = np.stack([vectorize_sum(text, embeddings_pretrained) for text in newsgroups_train.data])\n",
        "X_train_wv, X_test_wv, y_train, y_test = train_test_split(X_wv, newsgroups_train.target, test_size=0.2, random_state=0)\n",
        "X_train_wv.shape, X_test_wv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HniNyiVSKl8l",
        "outputId": "f1727b96-af42-4fa8-c18f-033366b9f7de"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1627, 25), (407, 25))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучаем модель логистической регрессии, считаем точность:"
      ],
      "metadata": {
        "id": "x-JWefX6fxeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = LogisticRegression(max_iter=5000)\n",
        "wv_model = clf.fit(X_train_wv, y_train)\n",
        "accuracy_score(y_test, wv_model.predict(X_test_wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoTZit_zKpFg",
        "outputId": "59316c44-52b9-4f24-8296-3b36cc50b32b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7027027027027027"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "clf = LogisticRegression(max_iter=10000)\n",
        "wv_model = clf.fit(X_train_wv, y_train)\n",
        "accuracy_score(y_test, wv_model.predict(X_test_wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOoI2HyVK-Ko",
        "outputId": "79afef8a-8a29-4605-a76c-ab7b1d296739"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7027027027027027"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}